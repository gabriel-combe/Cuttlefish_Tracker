% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{dai_r-fcn_2016}{misc}{}
      \name{author}{4}{}{%
        {{hash=c80dbb87ee6058c0b81ab8ee017f1170}{%
           family={Dai},
           familyi={D\bibinitperiod},
           given={Jifeng},
           giveni={J\bibinitperiod}}}%
        {{hash=cf1f0547c288eff0857572dcc12516bb}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod}}}%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
        {{hash=f85751488058842b5777c7b4074077b5}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{5e6d93db6ec8079f3e9ef43677e0e71c}
      \strng{fullhash}{d8450ebca907ed083f8e519d024f34bf}
      \strng{bibnamehash}{5e6d93db6ec8079f3e9ef43677e0e71c}
      \strng{authorbibnamehash}{5e6d93db6ec8079f3e9ef43677e0e71c}
      \strng{authornamehash}{5e6d93db6ec8079f3e9ef43677e0e71c}
      \strng{authorfullhash}{d8450ebca907ed083f8e519d024f34bf}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present region-based, fully convolutional networks for accurate and efﬁcient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [6, 18] that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classiﬁcation and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classiﬁer backbones, such as the latest Residual Networks (ResNets) [9], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6\% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20× faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.}
      \field{annotation}{Comment: Tech report}
      \field{month}{6}
      \field{note}{arXiv:1605.06409 [cs]}
      \field{shorttitle}{R-{FCN}}
      \field{title}{R-{FCN}: {Object} {Detection} via {Region}-based {Fully} {Convolutional} {Networks}}
      \field{year}{2016}
      \keyw{Computer Science - Computer Vision and Pattern Recognition}
    \endentry
    \entry{dalal_histograms_2005}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=521763e082baf91e4935f956c6c0f8b3}{%
           family={Dalal},
           familyi={D\bibinitperiod},
           given={N.},
           giveni={N\bibinitperiod}}}%
        {{hash=d90650ec8434180101eb4aeec8e4a0c6}{%
           family={Triggs},
           familyi={T\bibinitperiod},
           given={B.},
           giveni={B\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {San Diego, CA, USA}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{8316a8a4748fedae6ff9c5414cecc95a}
      \strng{fullhash}{8316a8a4748fedae6ff9c5414cecc95a}
      \strng{bibnamehash}{8316a8a4748fedae6ff9c5414cecc95a}
      \strng{authorbibnamehash}{8316a8a4748fedae6ff9c5414cecc95a}
      \strng{authornamehash}{8316a8a4748fedae6ff9c5414cecc95a}
      \strng{authorfullhash}{8316a8a4748fedae6ff9c5414cecc95a}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors signiﬁcantly outperform existing feature sets for human detection. We study the inﬂuence of each stage of the computation on performance, concluding that ﬁne-scale gradients, ﬁne orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.}
      \field{booktitle}{2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)}
      \field{title}{Histograms of {Oriented} {Gradients} for {Human} {Detection}}
      \field{volume}{1}
      \field{year}{2005}
      \field{pages}{886\bibrangedash 893}
      \range{pages}{8}
    \endentry
    \entry{pp2pf}{online}{}
      \name{author}{4}{}{%
        {{hash=f1cb584bef3901a9147d127dc802e491}{%
           family={Gabriel},
           familyi={G\bibinitperiod},
           given={Combe-Ounkham},
           giveni={C\bibinithyphendelim O\bibinitperiod}}}%
        {{hash=7cb599d7e32f385a3c1707a2c74384fd}{%
           family={Maxime},
           familyi={M\bibinitperiod},
           given={Beldjilali},
           giveni={B\bibinitperiod}}}%
        {{hash=9303aa9a00ce98ba225fc2c0e1019c9b}{%
           family={Luis-Miguel},
           familyi={L\bibinithyphendelim M\bibinitperiod},
           given={Young\bibnamedelima Brun},
           giveni={Y\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=7922c6155fa0676f25d56c9d15c4eed4}{%
           family={Ariane},
           familyi={A\bibinitperiod},
           given={Vaisse},
           giveni={V\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {GitHub}%
      }
      \strng{namehash}{14a5ce5326c41cc2bba4f0053c0cca9a}
      \strng{fullhash}{f49d75d069a5d60d4643fee890847850}
      \strng{bibnamehash}{14a5ce5326c41cc2bba4f0053c0cca9a}
      \strng{authorbibnamehash}{14a5ce5326c41cc2bba4f0053c0cca9a}
      \strng{authornamehash}{14a5ce5326c41cc2bba4f0053c0cca9a}
      \strng{authorfullhash}{f49d75d069a5d60d4643fee890847850}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{GitHub repository}
      \field{title}{PP2 Particle Filter}
      \field{year}{2023}
      \verb{urlraw}
      \verb https://github.com/gabriel-combe/PP2_Particle_Filter
      \endverb
      \verb{url}
      \verb https://github.com/gabriel-combe/PP2_Particle_Filter
      \endverb
    \endentry
    \entry{lin_focal_2018}{misc}{}
      \name{author}{5}{}{%
        {{hash=08f925fe4692d130a1d7cb7d94483351}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Tsung-Yi},
           giveni={T\bibinithyphendelim Y\bibinitperiod}}}%
        {{hash=d87be7c19fa16049347907c9820816c6}{%
           family={Goyal},
           familyi={G\bibinitperiod},
           given={Priya},
           giveni={P\bibinitperiod}}}%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross},
           giveni={R\bibinitperiod}}}%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
        {{hash=ecd149fdcb3e0503881d49e545744c3d}{%
           family={Dollár},
           familyi={D\bibinitperiod},
           given={Piotr},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{3b7e00bc88d7aca4d4d5ddb8c252e36c}
      \strng{fullhash}{4ccabcfdf9c34f3d9a88fdca5bb95dd5}
      \strng{bibnamehash}{3b7e00bc88d7aca4d4d5ddb8c252e36c}
      \strng{authorbibnamehash}{3b7e00bc88d7aca4d4d5ddb8c252e36c}
      \strng{authornamehash}{3b7e00bc88d7aca4d4d5ddb8c252e36c}
      \strng{authorfullhash}{4ccabcfdf9c34f3d9a88fdca5bb95dd5}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classiﬁer is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classiﬁed examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.}
      \field{month}{2}
      \field{note}{arXiv:1708.02002 [cs]}
      \field{title}{Focal {Loss} for {Dense} {Object} {Detection}}
      \field{year}{2018}
      \keyw{Computer Science - Computer Vision and Pattern Recognition}
    \endentry
    \entry{liu_ssd_2016}{incollection}{}
      \name{author}{7}{}{%
        {{hash=c0e0d23e2d09e45e6f51cc2bcea6d9f9}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod}}}%
        {{hash=c1826f3465579186aff299a9b0e16ed7}{%
           family={Anguelov},
           familyi={A\bibinitperiod},
           given={Dragomir},
           giveni={D\bibinitperiod}}}%
        {{hash=8bbc4c5d96f205bada839e74e0202146}{%
           family={Erhan},
           familyi={E\bibinitperiod},
           given={Dumitru},
           giveni={D\bibinitperiod}}}%
        {{hash=ed568d9c3bb059e6bf22899fbf170f86}{%
           family={Szegedy},
           familyi={S\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=698ee61a2f3fa29734204496d2d36aef}{%
           family={Reed},
           familyi={R\bibinitperiod},
           given={Scott},
           giveni={S\bibinitperiod}}}%
        {{hash=ec820780d594e36d11c6e30c7a2614e0}{%
           family={Fu},
           familyi={F\bibinitperiod},
           given={Cheng-Yang},
           giveni={C\bibinithyphendelim Y\bibinitperiod}}}%
        {{hash=963e9b2526a7150c418b4e9e9d19a82f}{%
           family={Berg},
           familyi={B\bibinitperiod},
           given={Alexander\bibnamedelima C.},
           giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \strng{namehash}{623b50f45b666c1e9b84e5228e255810}
      \strng{fullhash}{d5e1ac3dfe6687980f91e701611520ad}
      \strng{bibnamehash}{623b50f45b666c1e9b84e5228e255810}
      \strng{authorbibnamehash}{623b50f45b666c1e9b84e5228e255810}
      \strng{authornamehash}{623b50f45b666c1e9b84e5228e255810}
      \strng{authorfullhash}{d5e1ac3dfe6687980f91e701611520ad}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets conﬁrm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a uniﬁed framework for both training and inference. For 300 × 300 input, SSD achieves 74.3\% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9\% mAP, outperforming a comparable state-of-the-art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .}
      \field{annotation}{Comment: ECCV 2016}
      \field{shorttitle}{{SSD}}
      \field{title}{{SSD}: {Single} {Shot} {MultiBox} {Detector}}
      \field{volume}{9905}
      \field{year}{2016}
      \field{pages}{21\bibrangedash 37}
      \range{pages}{17}
      \keyw{Computer Science - Computer Vision and Pattern Recognition}
    \endentry
    \entry{qiang_zhu_fast_2006}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=095f784b151ef6e1ae92d614829f6e98}{%
           family={{Qiang Zhu}},
           familyi={Q\bibinitperiod}}}%
        {{hash=1950be5a07a5acf5c3474c6fd31f977f}{%
           family={{Mei-Chen Yeh}},
           familyi={M\bibinitperiod}}}%
        {{hash=4f893eccdd9d4c97ff19d5a0864862d7}{%
           family={{Kwang-Ting Cheng}},
           familyi={K\bibinitperiod}}}%
        {{hash=fe2d341c406b58a815582066e8375730}{%
           family={Avidan},
           familyi={A\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{1f56413fa0438ed7bc3c458757a54f82}
      \strng{fullhash}{5a5415d11d7fd7f0f34c1af0122a8919}
      \strng{bibnamehash}{1f56413fa0438ed7bc3c458757a54f82}
      \strng{authorbibnamehash}{1f56413fa0438ed7bc3c458757a54f82}
      \strng{authornamehash}{1f56413fa0438ed7bc3c458757a54f82}
      \strng{authorfullhash}{5a5415d11d7fd7f0f34c1af0122a8919}
      \field{sortinit}{Q}
      \field{sortinithash}{ce69a400a872ddd02ee7fdb3b38c6abd}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We integrate the cascade-of-rejectors approach with the Histograms of Oriented Gradients (HoG) features to achieve a fast and accurate human detection system. The features used in our system are HoGs of variable-size blocks that capture salient features of humans automatically. Using AdaBoost for feature selection, we identify the appropriate set of blocks, from a large set of possible blocks. In our system, we use the integral image representation and a rejection cascade which signiﬁcantly speed up the computation. For a 320 × 280 image, the system can process 5 to 30 frames per second depending on the density in which we scan the image, while maintaining an accuracy level similar to existing methods.}
      \field{booktitle}{2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Volume} 2 ({CVPR}'06)}
      \field{title}{Fast {Human} {Detection} {Using} a {Cascade} of {Histograms} of {Oriented} {Gradients}}
      \field{volume}{2}
      \field{year}{2006}
      \field{pages}{1491\bibrangedash 1498}
      \range{pages}{8}
    \endentry
    \entry{redmon_you_2016}{misc}{}
      \name{author}{4}{}{%
        {{hash=99bced2e56a5253f3fe98a5f04e6d9b2}{%
           family={Redmon},
           familyi={R\bibinitperiod},
           given={Joseph},
           giveni={J\bibinitperiod}}}%
        {{hash=05ca9f19da9ecbd2def4e5514f8043c8}{%
           family={Divvala},
           familyi={D\bibinitperiod},
           given={Santosh},
           giveni={S\bibinitperiod}}}%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross},
           giveni={R\bibinitperiod}}}%
        {{hash=396c6ddedb6f986906fc3e4994d19974}{%
           family={Farhadi},
           familyi={F\bibinitperiod},
           given={Ali},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{e1203a0044715040adeb8c5079ee645a}
      \strng{fullhash}{b5530443e433a4da53dbe3cf155225b4}
      \strng{bibnamehash}{e1203a0044715040adeb8c5079ee645a}
      \strng{authorbibnamehash}{e1203a0044715040adeb8c5079ee645a}
      \strng{authornamehash}{e1203a0044715040adeb8c5079ee645a}
      \strng{authorfullhash}{b5530443e433a4da53dbe3cf155225b4}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.}
      \field{month}{5}
      \field{shorttitle}{You {Only} {Look} {Once}}
      \field{title}{You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}}
      \field{year}{2016}
      \keyw{Computer Science - Computer Vision and Pattern Recognition}
    \endentry
    \entry{ren_faster_2016}{misc}{}
      \name{author}{4}{}{%
        {{hash=bb295293acacd54387339079ebbe4ead}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Shaoqing},
           giveni={S\bibinitperiod}}}%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross},
           giveni={R\bibinitperiod}}}%
        {{hash=f85751488058842b5777c7b4074077b5}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{f086ca4da3e532e8a41cb758ea461825}
      \strng{fullhash}{008a132af3e2d4ff15eb01a8fb4b005c}
      \strng{bibnamehash}{f086ca4da3e532e8a41cb758ea461825}
      \strng{authorbibnamehash}{f086ca4da3e532e8a41cb758ea461825}
      \strng{authornamehash}{f086ca4da3e532e8a41cb758ea461825}
      \strng{authorfullhash}{008a132af3e2d4ff15eb01a8fb4b005c}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fullyconvolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. The code will be released.}
      \field{annotation}{Comment: Extended tech report}
      \field{month}{1}
      \field{note}{arXiv:1506.01497 [cs]}
      \field{shorttitle}{Faster {R}-{CNN}}
      \field{title}{Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}}
      \field{year}{2016}
      \keyw{Computer Science - Computer Vision and Pattern Recognition}
    \endentry
    \entry{sanchez_review_2020}{article}{}
      \name{author}{3}{}{%
        {{hash=922f8c4141d245df4ab8bf56a3521283}{%
           family={Sanchez},
           familyi={S\bibinitperiod},
           given={S\bibnamedelima A},
           giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=99d39753cddb57293d04a98e8e835ca0}{%
           family={Romero},
           familyi={R\bibinitperiod},
           given={H\bibnamedelima J},
           giveni={H\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=0389850e74e9452e38613f6acdc047d8}{%
           family={Morales},
           familyi={M\bibinitperiod},
           given={A\bibnamedelima D},
           giveni={A\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \strng{namehash}{75218acc530dc7a653514a4a6610fbf4}
      \strng{fullhash}{75218acc530dc7a653514a4a6610fbf4}
      \strng{bibnamehash}{75218acc530dc7a653514a4a6610fbf4}
      \strng{authorbibnamehash}{75218acc530dc7a653514a4a6610fbf4}
      \strng{authornamehash}{75218acc530dc7a653514a4a6610fbf4}
      \strng{authorfullhash}{75218acc530dc7a653514a4a6610fbf4}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Advances in parallel computing, GPU technology and deep learning facilitate the tools for processing complex images. The purpose of this research was focused on a review of the state of the art, related to the performance of pre-trained models for the detection of objects in order to make a comparison of these algorithms in terms of reliability, ac- curacy, time processed and Problems detected The consulted models are based on the Python programming language, the use of libraries based on TensorFlow, OpenCv and free image databases (Microsoft COCO and PASCAL VOC 2007/2012). These systems are not only focused on the recognition and classification of the objects in the images, but also on the location of the objects within it, drawing a bounding box around the appropriate way. For this research, different pre-trained models were re- viewed for the detection of objects such as R-CNN, R-FCN, SSD (single- shot multibox) and YOLO (You Only Look Once), with different extractors of characteristics such as VGG16, ResNet, Inception, MobileNet. As a result, it is not prudent to make direct and parallel analyzes between the different architecture and models, because each case has a particular solution for each problem, the purpose of this research is to generate an approximate notion of the experiments that have been carried out and conceive a starting point in the use that they are intended to give.}
      \field{journaltitle}{IOP Conference Series: Materials Science and Engineering}
      \field{month}{6}
      \field{shorttitle}{A review}
      \field{title}{A review: {Comparison} of performance metrics of pretrained models for object detection using the {TensorFlow} framework}
      \field{volume}{844}
      \field{year}{2020}
      \field{pages}{012024}
      \range{pages}{1}
    \endentry
    \entry{yolov7_github}{online}{}
      \name{author}{3}{}{%
        {{hash=053d491932411950eccf2bb9a1025d11}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Chien-Yao},
           giveni={C\bibinithyphendelim Y\bibinitperiod}}}%
        {{hash=c0b2fdc20c37a4bd8d817822d374f809}{%
           family={Bochkovskiy},
           familyi={B\bibinitperiod},
           given={Alexey},
           giveni={A\bibinitperiod}}}%
        {{hash=c81478c691c1bc1cc01404c44b5bd5bf}{%
           family={Liao},
           familyi={L\bibinitperiod},
           given={Hong-Yuan\bibnamedelima Mark},
           giveni={H\bibinithyphendelim Y\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {GitHub}%
      }
      \strng{namehash}{9278ecb881789e377b71403c2cf54d74}
      \strng{fullhash}{9278ecb881789e377b71403c2cf54d74}
      \strng{bibnamehash}{9278ecb881789e377b71403c2cf54d74}
      \strng{authorbibnamehash}{9278ecb881789e377b71403c2cf54d74}
      \strng{authornamehash}{9278ecb881789e377b71403c2cf54d74}
      \strng{authorfullhash}{9278ecb881789e377b71403c2cf54d74}
      \field{extraname}{1}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{GitHub repository}
      \field{title}{YOLOv7}
      \field{year}{2022}
      \verb{urlraw}
      \verb https://github.com/WongKinYiu/yolov7
      \endverb
      \verb{url}
      \verb https://github.com/WongKinYiu/yolov7
      \endverb
    \endentry
    \entry{wang_yolov7_nodate}{article}{}
      \name{author}{3}{}{%
        {{hash=053d491932411950eccf2bb9a1025d11}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Chien-Yao},
           giveni={C\bibinithyphendelim Y\bibinitperiod}}}%
        {{hash=c0b2fdc20c37a4bd8d817822d374f809}{%
           family={Bochkovskiy},
           familyi={B\bibinitperiod},
           given={Alexey},
           giveni={A\bibinitperiod}}}%
        {{hash=c81478c691c1bc1cc01404c44b5bd5bf}{%
           family={Liao},
           familyi={L\bibinitperiod},
           given={Hong-Yuan\bibnamedelima Mark},
           giveni={H\bibinithyphendelim Y\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{9278ecb881789e377b71403c2cf54d74}
      \strng{fullhash}{9278ecb881789e377b71403c2cf54d74}
      \strng{bibnamehash}{9278ecb881789e377b71403c2cf54d74}
      \strng{authorbibnamehash}{9278ecb881789e377b71403c2cf54d74}
      \strng{authornamehash}{9278ecb881789e377b71403c2cf54d74}
      \strng{authorfullhash}{9278ecb881789e377b71403c2cf54d74}
      \field{extraname}{2}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWINL Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutionalbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https:// github.com/ WongKinYiu/ yolov7.}
      \field{title}{{YOLOv7}: {Trainable} bag-of-freebies sets new state-of-the-art for real-time object detectors}
    \endentry
  \enddatalist
\endrefsection
\endinput

