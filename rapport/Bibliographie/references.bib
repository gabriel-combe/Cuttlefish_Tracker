
@inproceedings{xu_human_2010,
	address = {Yantai, China},
	title = {Human detection and tracking based on {HOG} and particle filter},
	abstract = {Human detection and tracking is a task common to many applications, such as video surveillance and security, intelligent vehicles, safety driving, public security, etc. Histogram of oriented gradient (HOG) gives an accurate description of the contour of human body. Based on HOG and support vector machine (SVM) theory, a classifier for pedestrian is obtained. The classifier is then used to find the potential human candidate in the video frame. By calculating the similarity between particle candidates and the target model using Bhattacharyya Coefficient, a tracking algorithm using particle filter is designed and implemented. Experimental results show that the proposed algorithm out-performs Kalman filter based tracking in almost all situations, especially when partial occlusion of object is present.},
	booktitle = {2010 3rd {International} {Congress} on {Image} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Xu, Fen and Gao, Ming},
	month = oct,
	year = {2010},
	pages = {1503--1507}
}

@article{kong_particle_2019,
	title = {Particle filter‐based vehicle tracking via {HOG} features after image stabilisation in intelligent drive system},
	volume = {13},
	abstract = {Tracking vehicles automatically are of great importance in the intelligent transportation systems to guarantee the traffic safety, especially for the self-driving technique. To solve the problem of the tiny jitter of the camera due to the shaking platform and the rough road, and to track the vehicle ahead, a salient feature-based video stabilisation method is introduced to remove the effect of camera motion, and an improved particle filter-based vehicle tracking algorithm via histogram of oriented gradient (HOG) features is proposed in this study. For video stabilisation, first, the features from accelerated segment test algorithm are applied to extract salient points from the two adjacent frames. Then, the fast retina keypoint algorithm is used to match the correspondences and the M-estimator sample consensus algorithm is applied to remove the outliers. Finally, the image with tiny jitter is stabilised by the affine transformation matrix calculated from the retained inliers. After the image stabilisation, an improved particle filter-based tracking method is proposed for vehicle tracking via the HOG feature extracted from an adaptive searching area determined by the expected vehicle position of the previous frame. Experimental results demonstrate that camera motion can be effectively compensated by the feature-based stabilisation method, and the forward vehicle can be tracked with stability and robustness in real time.},
	number = {6},
	journal = {IET Intelligent Transport Systems},
	author = {Kong, Xiaofang and Chen, Qian and Gu, Guohua and Ren, Kan and Qian, Weixian and Liu, Zewei},
	month = jun,
	year = {2019},
	pages = {942--949}
}

@inproceedings{qiang_zhu_fast_2006,
	address = {New York, NY, USA},
	title = {Fast {Human} {Detection} {Using} a {Cascade} of {Histograms} of {Oriented} {Gradients}},
	volume = {2},
	abstract = {We integrate the cascade-of-rejectors approach with the Histograms of Oriented Gradients (HoG) features to achieve a fast and accurate human detection system. The features used in our system are HoGs of variable-size blocks that capture salient features of humans automatically. Using AdaBoost for feature selection, we identify the appropriate set of blocks, from a large set of possible blocks. In our system, we use the integral image representation and a rejection cascade which signiﬁcantly speed up the computation. For a 320 × 280 image, the system can process 5 to 30 frames per second depending on the density in which we scan the image, while maintaining an accuracy level similar to existing methods.},
	booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Volume} 2 ({CVPR}'06)},
	publisher = {IEEE},
	author = {{Qiang Zhu} and {Mei-Chen Yeh} and {Kwang-Ting Cheng} and Avidan, S.},
	year = {2006},
	pages = {1491--1498}
}

@inproceedings{dalal_histograms_2005,
	address = {San Diego, CA, USA},
	title = {Histograms of {Oriented} {Gradients} for {Human} {Detection}},
	volume = {1},
	abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors signiﬁcantly outperform existing feature sets for human detection. We study the inﬂuence of each stage of the computation on performance, concluding that ﬁne-scale gradients, ﬁne orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	publisher = {IEEE},
	author = {Dalal, N. and Triggs, B.},
	year = {2005},
	pages = {886--893}
}

@article{bhattacharyya_measure_1960,
	title = {On a {Measure} of {Divergence} between {Two} {Multinomial} {Populations}},
	volume = {7},
	number = {4},
	journal = {Sankhyā: The Indian Journal of Statistics (1933-1960)},
	author = {Bhattacharyya, A.},
	year = {1960},
	pages = {401--406}
}

@ONLINE{rlabbe,
	author = {Roger Labbe (rlabbe)},
	title = {Kalman and Bayesian Filters in Python},
	year = {2014},
	publisher = {GitHub},
	journal = {GitHub repository},
	url = {https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python}
}

@article{wang_yolov7_nodate,
	title = {{YOLOv7}: {Trainable} bag-of-freebies sets new state-of-the-art for real-time object detectors},
	abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWINL Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutionalbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https:// github.com/ WongKinYiu/ yolov7.},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark}
}

@incollection{liu_ssd_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	shorttitle = {{SSD}},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets conﬁrm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a uniﬁed framework for both training and inference. For 300 × 300 input, SSD achieves 74.3\% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9\% mAP, outperforming a comparable state-of-the-art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {21--37},
	annote = {Comment: ECCV 2016}
}

@misc{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	publisher = {arXiv},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = may,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@ONLINE{pp2pf,
	author = {Gabriel, Combe-Ounkham and Maxime, Beldjilali and Luis-Miguel, Young Brun and Ariane, Vaisse},
	title = {Cuttlefish Tracker},
	year = {2023},
	publisher = {GitHub},
	journal = {GitHub repository},
	url = {https://github.com/gabriel-combe/Cuttlefish_Tracker}
}

@ONLINE{yolov7_github,
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	title = {YOLOv7},
	year = {2022},
	publisher = {GitHub},
	journal = {GitHub repository},
	url = {https://github.com/WongKinYiu/yolov7}
}

@article{sanchez_review_2020,
	title = {A review: {Comparison} of performance metrics of pretrained models for object detection using the {TensorFlow} framework},
	volume = {844},
	shorttitle = {A review},
	abstract = {Advances in parallel computing, GPU technology and deep learning facilitate the tools for processing complex images. The purpose of this research was focused on a review of the state of the art, related to the performance of pre-trained models for the detection of objects in order to make a comparison of these algorithms in terms of reliability, ac- curacy, time processed and Problems detected The consulted models are based on the Python programming language, the use of libraries based on TensorFlow, OpenCv and free image databases (Microsoft COCO and PASCAL VOC 2007/2012). These systems are not only focused on the recognition and classification of the objects in the images, but also on the location of the objects within it, drawing a bounding box around the appropriate way. For this research, different pre-trained models were re- viewed for the detection of objects such as R-CNN, R-FCN, SSD (single- shot multibox) and YOLO (You Only Look Once), with different extractors of characteristics such as VGG16, ResNet, Inception, MobileNet. As a result, it is not prudent to make direct and parallel analyzes between the different architecture and models, because each case has a particular solution for each problem, the purpose of this research is to generate an approximate notion of the experiments that have been carried out and conceive a starting point in the use that they are intended to give.},
	journal = {IOP Conference Series: Materials Science and Engineering},
	author = {Sanchez, S A and Romero, H J and Morales, A D},
	month = jun,
	year = {2020},
	pages = {012024}
}

@misc{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fullyconvolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. The code will be released.},
	publisher = {arXiv},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv:1506.01497 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended tech report}
}

@misc{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classiﬁer is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classiﬁed examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv:1708.02002 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012—achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-speciﬁc ﬁne-tuning, yields a signiﬁcant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We ﬁnd that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/˜rbg/rcnn.},
	publisher = {arXiv},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv:1311.2524 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)}
}

@misc{dai_r-fcn_2016,
	title = {R-{FCN}: {Object} {Detection} via {Region}-based {Fully} {Convolutional} {Networks}},
	shorttitle = {R-{FCN}},
	abstract = {We present region-based, fully convolutional networks for accurate and efﬁcient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [6, 18] that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classiﬁcation and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classiﬁer backbones, such as the latest Residual Networks (ResNets) [9], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6\% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20× faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.},
	publisher = {arXiv},
	author = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
	month = jun,
	year = {2016},
	note = {arXiv:1605.06409 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report}
}

@book{russell_norvig,
	chapter=14,
	pages={509-517},
	year=2021,
	title={Artificial Intelligence : A Modern Approach},
	isbn={1-292-40113-3},
	author={Russel, Stuart and Norvig, Peter},
	publisher={Pearson}
}

